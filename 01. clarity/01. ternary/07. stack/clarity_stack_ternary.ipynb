{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "###################################################################################################\n",
    "#Auxiliares\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "from time import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "# Now using tensorflow 2.1.0, so no need to patch\n",
    "# from tfdeterminism import patch\n",
    "#patch()\n",
    "\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "###################################################################################################\n",
    "# utils\n",
    "from distutils.version import LooseVersion\n",
    "from tqdm import tqdm_notebook\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "import warnings\n",
    "import pickle\n",
    "import gc\n",
    "import sys\n",
    "from json import dumps\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "# Data\n",
    "import spacy\n",
    "\n",
    "# Viz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow.keras.backend as K\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dropout, Bidirectional, LSTM, Flatten, Dense, Reshape\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks.callbacks import EarlyStopping\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from keras_multi_head import MultiHead, MultiHeadAttention\n",
    "###################################################################################################\n",
    "#Dados\n",
    "import pandas as pd\n",
    "import matplotlib as pl\n",
    "\n",
    "#preprocessing and transformation\n",
    "from sklearn.preprocessing import normalize, MaxAbsScaler, MinMaxScaler, StandardScaler\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk import word_tokenize, pos_tag\n",
    "\n",
    "#Machine learning\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer, HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_curve, auc, precision_recall_fscore_support, f1_score\n",
    "\n",
    "seed = 42\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['pid', 'req-text', 'resp-text', '3ppron-request', '4i-request',\n",
      "       '5we-request', '6you-request', '7shehe-request', '8they-request',\n",
      "       '9ipron-request',\n",
      "       ...\n",
      "       'sentence_length_max', 'sentence_length_min',\n",
      "       'sentence_length_standard_deviation', 'short_sentence_ratio',\n",
      "       'std_noun_phrase', 'verb_diversity', 'verbs_max', 'verbs_min',\n",
      "       'verbs_standard_deviation', 'long_sentence_ratio'],\n",
      "      dtype='object', length=310)\n",
      "(2664, 310)\n",
      "2    1188\n",
      "1     773\n",
      "0     703\n",
      "Name: Clareza, dtype: int64\n",
      "2    1188\n",
      "1     773\n",
      "0     703\n",
      "Name: Clareza, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Variables\n",
    "label='Clareza'\n",
    "exp = label + '-Balanced-Multiclass'\n",
    "base_path = 'D:/03. Documentos/Mestrado/22032020 - Experimentos/05. Organizado/03. Datasets/'+exp\n",
    "save_path = 'output'\n",
    "data='resp-text'\n",
    "\n",
    "# Models to be loaded\n",
    "reglog_model_path = 'D:/03. Documentos/Mestrado/22032020 - Experimentos/05. Organizado/02. Notebooks/02. '+label+'/03. Resposta/output'\n",
    "reglog_model_name = '2020_04_19_22_44_04_Clareza_Resposta_Multiclass_Balanced_word.sav'\n",
    "bilstm_mha_model_path = 'D:/Outputs_Mestrado/resultados_Clareza/checkpoins_resposta_keras_mh_att'\n",
    "bilstm_mha_model_name = '20200520_215052/model.h5'\n",
    "\n",
    "x_train_file = 'X_train.csv'\n",
    "y_train_file = 'y_train.csv'\n",
    "x_test_file = 'X_test.csv'\n",
    "y_test_file = 'y_test.csv'\n",
    "\n",
    "#Load data\n",
    "X_train = pd.read_csv(os.path.join(base_path, x_train_file), sep=';', encoding='utf-8')\n",
    "y_train = pd.read_csv(os.path.join(base_path, y_train_file), sep=';', encoding='utf-8')\n",
    "X_test = pd.read_csv(os.path.join(base_path, x_test_file), sep=';', encoding='utf-8')\n",
    "y_test = pd.read_csv(os.path.join(base_path, y_test_file), sep=';', encoding='utf-8')\n",
    "\n",
    "#Checking on data\n",
    "print(X_test.columns)\n",
    "print(X_test.shape)\n",
    "print(y_test[label].value_counts())\n",
    "print(y_test[label].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the RegLog model from disk\n",
    "reglog_model = pickle.load(open(os.path.join(reglog_model_path, reglog_model_name), 'rb'))\n",
    "bilstm_mha = load_model(os.path.join(bilstm_mha_model_path, bilstm_mha_model_name),\n",
    "                        custom_objects={'MultiHeadAttention': MultiHeadAttention,\n",
    "                                        'recall_m': recall_m,\n",
    "                                        'precision_m': precision_m,\n",
    "                                        'f1_m': f1_m})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_words = bilstm_mha._layers[1]._trainable_weights[0].shape[0]\n",
    "max_length = bilstm_mha.inputs[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICT TRAIN\n",
    "y_pred_reglog_train = reglog_model.predict(X_train[data])\n",
    "y_pred_reglog_prob_train = reglog_model.predict_proba(X_train[data])\n",
    "\n",
    "# PREDICT TEST\n",
    "y_pred_reglog_test = reglog_model.predict(X_test[data])\n",
    "y_pred_reglog_prob_test = reglog_model.predict_proba(X_test[data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 33506 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Define tokenizer and fit train data\n",
    "t = Tokenizer(num_words=max_num_words)\n",
    "t.fit_on_texts(X_train[data].append(X_test[data]))\n",
    "word_index = t.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "    \n",
    "def get_seqs(text):    \n",
    "    sequences = t.texts_to_sequences(text)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "    return padded_sequences\n",
    "\n",
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y_train)\n",
    "    y_train_enc = le.transform(y_train)\n",
    "    y_test_enc = le.transform(y_test)\n",
    "    return pd.get_dummies(y_train_enc), pd.get_dummies(y_test_enc)\n",
    "\n",
    "# X and Y\n",
    "label_train, label_test = prepare_targets(y_train[label].values, y_test[label].values)\n",
    "num_labels = len(set(label_train))\n",
    "input_train = get_seqs(X_train[data])\n",
    "input_test = get_seqs(X_test[data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICT TRAIN\n",
    "y_pred_bilstm_mha_prob_train = bilstm_mha.predict(input_train)\n",
    "y_pred_bilstm_mha_train = np.argmax(y_pred_bilstm_mha_prob_train, axis=1)\n",
    "\n",
    "# PREDICT TEST\n",
    "y_pred_bilstm_mha_prob_test = bilstm_mha.predict(input_test)\n",
    "y_pred_bilstm_mha_test = np.argmax(y_pred_bilstm_mha_prob_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('clf', LogisticRegression(random_state=seed, max_iter=1000, n_jobs=6, solver='lbfgs'))\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "        'clf__C': (0.001, 0.01, 0.1, 1, 10, 100, 1000),\n",
    "}\n",
    "\n",
    "grid_search_ensemble = GridSearchCV(pipe,\n",
    "                               parameters,\n",
    "                               cv=10,\n",
    "                               scoring='f1_macro',\n",
    "                               n_jobs=-1,\n",
    "                               verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reglog_0</th>\n",
       "      <th>reglog_1</th>\n",
       "      <th>reglog_2</th>\n",
       "      <th>bilstm_mha_0</th>\n",
       "      <th>bilstm_mha_1</th>\n",
       "      <th>bilstm_mha_2</th>\n",
       "      <th>Clareza</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.999747</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.269941</td>\n",
       "      <td>0.505364</td>\n",
       "      <td>0.224695</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001703</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.997413</td>\n",
       "      <td>0.491280</td>\n",
       "      <td>0.306884</td>\n",
       "      <td>0.201837</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.999339</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.321680</td>\n",
       "      <td>0.493293</td>\n",
       "      <td>0.185027</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002613</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>0.995229</td>\n",
       "      <td>0.258270</td>\n",
       "      <td>0.427935</td>\n",
       "      <td>0.313795</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.999422</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.134660</td>\n",
       "      <td>0.382263</td>\n",
       "      <td>0.483077</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reglog_0  reglog_1  reglog_2  bilstm_mha_0  bilstm_mha_1  bilstm_mha_2  \\\n",
       "0  0.000108  0.999747  0.000145      0.269941      0.505364      0.224695   \n",
       "1  0.001703  0.000884  0.997413      0.491280      0.306884      0.201837   \n",
       "2  0.000585  0.999339  0.000077      0.321680      0.493293      0.185027   \n",
       "3  0.002613  0.002158  0.995229      0.258270      0.427935      0.313795   \n",
       "4  0.000274  0.999422  0.000304      0.134660      0.382263      0.483077   \n",
       "\n",
       "   Clareza  \n",
       "0        1  \n",
       "1        2  \n",
       "2        1  \n",
       "3        2  \n",
       "4        1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prob_reglog_train = pd.DataFrame(y_pred_reglog_prob_train, columns=['reglog_0', 'reglog_1', 'reglog_2'])\n",
    "df_prob_bilstm_mha_train = pd.DataFrame(y_pred_bilstm_mha_prob_train, columns=['bilstm_mha_0', 'bilstm_mha_1', 'bilstm_mha_2'])\n",
    "df_y_train = pd.DataFrame(y_train[label], columns=[label]).reset_index(drop=True)\n",
    "df_probs_train = pd.concat([df_prob_reglog_train, df_prob_bilstm_mha_train, df_y_train], axis=1)\n",
    "\n",
    "df_probs_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executando Gridsearch para Classe Clareza - resp-text\n",
      "20200607_111656\n",
      "done in 3.477s\n",
      "Best score: 0.840\n",
      "Best parameters set:\n",
      "\tclf__C: 0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Executando Gridsearch para Classe \" + label + \" - \" + data)\n",
    "\n",
    "# Time now\n",
    "now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(now)\n",
    "\n",
    "t0 = time()\n",
    "grid_search_ensemble.fit(df_probs_train.iloc[:, :-1], df_probs_train[label])\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Best score: %0.3f\" % grid_search_ensemble.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters_ensemble = grid_search_ensemble.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters_ensemble[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_save = now + '_Ensemble_LR_' + exp + '_' + label + '.sav'\n",
    "pickle.dump(grid_search_ensemble, open(f_save,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reglog_0</th>\n",
       "      <th>reglog_1</th>\n",
       "      <th>reglog_2</th>\n",
       "      <th>bilstm_mha_0</th>\n",
       "      <th>bilstm_mha_1</th>\n",
       "      <th>bilstm_mha_2</th>\n",
       "      <th>Clareza</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.831798</td>\n",
       "      <td>0.073774</td>\n",
       "      <td>0.094427</td>\n",
       "      <td>0.571900</td>\n",
       "      <td>0.134328</td>\n",
       "      <td>0.293772</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.156689</td>\n",
       "      <td>0.013231</td>\n",
       "      <td>0.830080</td>\n",
       "      <td>0.480867</td>\n",
       "      <td>0.201737</td>\n",
       "      <td>0.317396</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.018275</td>\n",
       "      <td>0.180967</td>\n",
       "      <td>0.800758</td>\n",
       "      <td>0.373061</td>\n",
       "      <td>0.340149</td>\n",
       "      <td>0.286790</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.029324</td>\n",
       "      <td>0.004981</td>\n",
       "      <td>0.965695</td>\n",
       "      <td>0.222213</td>\n",
       "      <td>0.238187</td>\n",
       "      <td>0.539600</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276271</td>\n",
       "      <td>0.633760</td>\n",
       "      <td>0.089968</td>\n",
       "      <td>0.265337</td>\n",
       "      <td>0.320776</td>\n",
       "      <td>0.413887</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reglog_0  reglog_1  reglog_2  bilstm_mha_0  bilstm_mha_1  bilstm_mha_2  \\\n",
       "0  0.831798  0.073774  0.094427      0.571900      0.134328      0.293772   \n",
       "1  0.156689  0.013231  0.830080      0.480867      0.201737      0.317396   \n",
       "2  0.018275  0.180967  0.800758      0.373061      0.340149      0.286790   \n",
       "3  0.029324  0.004981  0.965695      0.222213      0.238187      0.539600   \n",
       "4  0.276271  0.633760  0.089968      0.265337      0.320776      0.413887   \n",
       "\n",
       "   Clareza  \n",
       "0        0  \n",
       "1        1  \n",
       "2        1  \n",
       "3        2  \n",
       "4        1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prob_reglog_test = pd.DataFrame(y_pred_reglog_prob_test, columns=['reglog_0', 'reglog_1', 'reglog_2'])\n",
    "df_prob_bilstm_mha_test = pd.DataFrame(y_pred_bilstm_mha_prob_test, columns=['bilstm_mha_0', 'bilstm_mha_1', 'bilstm_mha_2'])\n",
    "df_y_test = pd.DataFrame(y_test[label], columns=[label]).reset_index(drop=True)\n",
    "df_probs_test = pd.concat([df_prob_reglog_test, df_prob_bilstm_mha_test, df_y_test], axis=1)\n",
    "\n",
    "df_probs_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ensemble = grid_search_ensemble.predict(df_probs_test.iloc[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "Report for TEST\n",
      "##################################################\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.35      0.36       703\n",
      "           1       0.31      0.30      0.31       773\n",
      "           2       0.49      0.51      0.50      1188\n",
      "\n",
      "    accuracy                           0.41      2664\n",
      "   macro avg       0.39      0.39      0.39      2664\n",
      "weighted avg       0.41      0.41      0.41      2664\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('#'*50)\n",
    "print('Report for TEST')\n",
    "print('#'*50)\n",
    "print(classification_report(df_probs_test.iloc[:,-1], y_pred_ensemble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_env",
   "language": "python",
   "name": "keras_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
