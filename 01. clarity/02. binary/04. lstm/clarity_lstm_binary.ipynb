{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow-gpu\n",
    "# Downgrade \n",
    "#smart_open to 1.10.0 -> https://github.com/RaRe-Technologies/smart_open/issues/475\n",
    "# python -m pip install -U smart_open==1.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# How to make deterministic experiments?\n",
    "###########################################\n",
    "\n",
    "# Main Sources:\n",
    "    # 1) https://github.com/NVIDIA/tensorflow-determinism\n",
    "    # 2) https://pypi.org/project/tensorflow-determinism/#description\n",
    "            # There are currently two main ways to access GPU-deterministic functionality in TensorFlow for most\n",
    "            # deep learning applications. \n",
    "            # 2.1) The first way is to use an NVIDIA NGC TensorFlow container. - https://www.nvidia.com/en-us/gpu-cloud/containers/\n",
    "            # 2.2. The second way is to use version 1.14, 1.15, or 2.0 of stock TensorFlow with GPU support, \n",
    "            #      plus the application of a patch supplied in this repo.\n",
    "\n",
    "# # # Ensure Deterministic behaviour\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "# Now using tensorflow 2.1.0, so no need to patch\n",
    "# from tfdeterminism import patch\n",
    "#patch()\n",
    "\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "from distutils.version import LooseVersion\n",
    "from tqdm import tqdm_notebook\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "import warnings\n",
    "import pickle\n",
    "import gc\n",
    "import sys\n",
    "from json import dumps\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "# Data\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Viz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow.keras.backend as K\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dropout, Bidirectional, LSTM, Flatten, Dense, Reshape\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks.callbacks import EarlyStopping\n",
    "#from keras.callbacks import EarlyStopping, TensorBoard\n",
    "#from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from keras_multi_head import MultiHead, MultiHeadAttention\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# NLP Models\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "#w2v_models_path = 'C:/Users/arthu/Desktop/22032020 - Experimentos/05. Organizado/02. Notebooks/models/'\n",
    "w2v_models_path = 'D:/03. Documentos/Mestrado/Dissertação/07 .Dissertação Final/02. Experimentos/02. Word Embbedings/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS\n",
    "# def f1_score(y_true, y_pred):\n",
    "\n",
    "#     # Count positive samples.\n",
    "#     c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "#     c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "#     c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "# #     # If there are no true samples, fix the F1 score at 0.\n",
    "# #     if c3 == 0:\n",
    "# #         return 0\n",
    "\n",
    "#     # How many selected items are relevant?\n",
    "#     precision = c1 / c2\n",
    "\n",
    "#     # How many relevant items are selected?\n",
    "#     recall = c1 / c3\n",
    "\n",
    "#     # Calculate f1_score\n",
    "#     f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "#     return f1_score\n",
    "\n",
    "# def recall(y_true, y_pred):\n",
    "\n",
    "#     # Count positive samples.\n",
    "#     c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "#     c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "# #     # If there are no true samples, fix the F1 score at 0.\n",
    "# #     if c3 == 0:\n",
    "# #         return 0\n",
    "\n",
    "#     # How many relevant items are selected?\n",
    "#     recall = c1 / c3\n",
    "    \n",
    "#     return recall\n",
    "\n",
    "\n",
    "# def precision(y_true, y_pred):\n",
    "\n",
    "#     # Count positive samples.\n",
    "#     c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "#     c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "\n",
    "#     # How many selected items are relevant?\n",
    "#     precision = c1 / c2\n",
    "\n",
    "#     return precision\n",
    "\n",
    "# https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('tf version: ' + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables\n",
    "current_exp = 'Clareza-Unbalanced-Binary-COH-METRIX'\n",
    "if 'Binary' in current_exp:\n",
    "    binary = True\n",
    "else:\n",
    "    binary = False\n",
    "    \n",
    "base_path = 'D:/03. Documentos/Mestrado/22032020 - Experimentos/05. Organizado/03. Datasets/' + current_exp\n",
    "save_path = 'output'\n",
    "\n",
    "sentence = 'resp-text'\n",
    "label = 'Clareza'\n",
    "\n",
    "x_train_file = 'X_train.csv'\n",
    "y_train_file = 'y_train.csv'\n",
    "x_test_file = 'X_test.csv'\n",
    "y_test_file = 'y_test.csv'\n",
    "\n",
    "#Load data\n",
    "X_train = pd.read_csv(os.path.join(base_path, x_train_file), sep=';', encoding='utf-8')\n",
    "y_train = pd.read_csv(os.path.join(base_path, y_train_file), sep=';', encoding='utf-8')\n",
    "X_test = pd.read_csv(os.path.join(base_path, x_test_file), sep=';', encoding='utf-8')\n",
    "y_test = pd.read_csv(os.path.join(base_path, y_test_file), sep=';', encoding='utf-8')\n",
    "\n",
    "#Checking on data\n",
    "print(X_train.columns)\n",
    "print(X_train.shape)\n",
    "print(y_train[label].value_counts())\n",
    "print(y_test[label].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only text columns\n",
    "X_train.drop(columns=X_train.columns[3:], inplace=True)\n",
    "X_test.drop(columns=X_train.columns[3:], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "X_train['sentence'] = X_train[sentence]\n",
    "X_test['sentence'] = X_test[sentence]\n",
    "\n",
    "y_train['label'] = y_train[label]\n",
    "y_test['label'] = y_test[label]\n",
    "\n",
    "##################################################################\n",
    "# CUT DATAFRAME\n",
    "# factor = 10000\n",
    "# df = pd.concat([df[df.label=='1'][0:factor], df[df.label=='0'][0:factor]])\n",
    "##################################################################\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(X_train.shape[0]))\n",
    "\n",
    "# Report the classes balance.\n",
    "print('Classes distribuition: \\n')\n",
    "print(y_train[label].value_counts())\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "X_train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking lengths\n",
    "lengths = [X_train.sentence.apply(lambda x: len(x.split(' ')))]\n",
    "perc =[.25, .50, .75, .80, .85, .90, .91, .92, .93, .94, .95, .96, .97, .98, .99] \n",
    "lengths[0].describe(percentiles = perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w2v Model for Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v_cbow_esic_model=KeyedVectors.load(os.path.join(w2v_models_path,'word2vec_sg_hs_DetalhamentoSolicitacao_all_sentences_128.model'))\n",
    "w2v_cbow_nilc_model=KeyedVectors.load_word2vec_format(os.path.join(w2v_models_path,'cbow_s300.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = w2v_cbow_nilc_model.wv.syn0\n",
    "print(pretrained_weights.shape)\n",
    "max_num_words = pretrained_weights.shape[0]\n",
    "embed_size = pretrained_weights.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=128\n",
    "\n",
    "# Define tokenizer and fit train data\n",
    "t = Tokenizer(num_words=max_num_words)\n",
    "t.fit_on_texts(X_train['sentence'].append(X_test['sentence']))\n",
    "word_index = t.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "    \n",
    "def get_seqs(text):    \n",
    "    sequences = t.texts_to_sequences(text)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y_train)\n",
    "    y_train_enc = le.transform(y_train)\n",
    "    y_test_enc = le.transform(y_test)\n",
    "    #if binary:\n",
    "    #    return y_train_enc, y_test_enc\n",
    "    #else:\n",
    "    return pd.get_dummies(y_train_enc), pd.get_dummies(y_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and Y\n",
    "label_train, label_test = prepare_targets(y_train.label.values, y_test.label.values)\n",
    "num_labels = len(set(label_train))\n",
    "input_train = get_seqs(X_train.sentence)\n",
    "input_test = get_seqs(X_test.sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, embed_size))\n",
    "for word, i in t.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = w2v_cbow_nilc_model.wv.__getitem__(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    # Words not in vocab -> Frequency less than 5 word\n",
    "    except KeyError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "def train_model(input_train, input_test, label_train, label_test,\n",
    "                lstm_size=128, dropout=0.2, rec_dropout=0.2, lr=0.005, epochs=50, att_heads=4, max_length=128, \n",
    "                vocab_size=None, embed_size=None, emb_trainable=False, batch=128, early_stopping=5,\n",
    "                save_dir=\"D:/resultados/checkpoins_solicitacao_keras_mh_att/\", best_predefined_f1=0.390):\n",
    "\n",
    "    # Time now\n",
    "    now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Log\n",
    "    log_dir = save_dir + now\n",
    "    \n",
    "    # Model Saver    \n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    \n",
    "    log_file = open(os.path.join(log_dir,\"log.txt\"), mode=\"a\")\n",
    "    \n",
    "    # Save Params\n",
    "    params = {\n",
    "        'lstm_size': lstm_size,\n",
    "        'dropout': dropout,\n",
    "        'rec_dropout': rec_dropout,\n",
    "        'lr': lr,\n",
    "        'epochs': epochs,\n",
    "        'att_heads': att_heads,\n",
    "        'max_length': max_length,\n",
    "        'vocab_size': vocab_size,\n",
    "        'embed_size': embed_size,\n",
    "        'emb_trainable': emb_trainable,\n",
    "        'batch': batch,\n",
    "        'early_stopping': early_stopping,\n",
    "        'log_dir': log_dir,\n",
    "        'best_predefined_f1': best_predefined_f1}\n",
    "    \n",
    "    # Saving Parameters\n",
    "    with open(os.path.join(log_dir, 'params.txt'),'a') as f:\n",
    "        f.write('\\n\\n' + ('#'*60))\n",
    "        f.write('\\nParameters:\\n')\n",
    "        f.write('now: ' + str(now))\n",
    "        f.write('\\n' + dumps(params) + '\\n')\n",
    "    \n",
    "    # input\n",
    "    inp = Input(shape=(max_length, ))\n",
    "\n",
    "    # Embedding layer - https://keras.io/layers/embeddings/\n",
    "    embedding_layer = Embedding(vocab_size,\n",
    "                                embed_size,                                \n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_length,\n",
    "                                trainable=emb_trainable,\n",
    "                                name='Embedding')(inp)\n",
    "\n",
    "    # Bidirectional Layer\n",
    "    bilstm_layer = Bidirectional(LSTM(\n",
    "                        units=lstm_size,\n",
    "                        return_sequences=True,\n",
    "                        dropout=dropout,\n",
    "                        recurrent_dropout=rec_dropout,\n",
    "                        name='LSTM'))(embedding_layer)    \n",
    "\n",
    "    # MultiHead-Attention Layer\n",
    "    #https://pypi.org/project/keras-multi-head/\n",
    "    multiHead_att_layer = MultiHeadAttention(head_num=att_heads, name='Multi-Head-Attention')(bilstm_layer)\n",
    "\n",
    "    dropout_intermed_layer = Dropout(0.5)(multiHead_att_layer)\n",
    "\n",
    "    # # Flatten\n",
    "    flatten_layer = Flatten(name='Flatten')(dropout_intermed_layer)\n",
    "\n",
    "    #dense_intermed_layer = Dense(128, activation='relu')(flatten_layer)\n",
    "    #dropout_intermed_2_layer = Dropout(dropout)(dense_intermed_layer)\n",
    "\n",
    "    # # # # Dense Layer\n",
    "    #if binary:\n",
    "    #    dense_layer = Dense(1, activation='sigmoid')(flatten_layer)    \n",
    "    #else:\n",
    "    dense_layer = Dense(num_labels, activation='softmax')(flatten_layer)    \n",
    "    \n",
    "    model = Model(inputs=inp, outputs=dense_layer)\n",
    "    # model.summary()\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(optimizer=Adam(lr=lr), loss='binary_crossentropy', metrics=['accuracy', precision_m, recall_m, f1_m])\n",
    "    \n",
    "    # callbacks\n",
    "    es_callback = EarlyStopping(monitor='val_loss', patience=early_stopping, verbose=1, mode='min')\n",
    "    \n",
    "    # Fitting Model\n",
    "    model.fit(input_train,\n",
    "              label_train,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch,\n",
    "              validation_data=(input_test, label_test),\n",
    "              verbose=0,\n",
    "              callbacks=[es_callback])\n",
    "    \n",
    "    # PLOT LOSS\n",
    "    plt.title('Loss')\n",
    "    plt.plot(model.history.history['loss'], label='train')\n",
    "    plt.plot(model.history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    #plt.show();\n",
    "    plt.savefig(os.path.join(log_dir,'Loss.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Classification\n",
    "    y_pred = model.predict(input_test, batch_size=batch, verbose=1)\n",
    "    y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    #if not binary:\n",
    "    label_test = np.argmax(label_test.values, axis=1)\n",
    "    \n",
    "    # Metrics\n",
    "    f1 = f1_score(label_test, y_pred_bool, average='weighted')\n",
    "    print(f\"Best Test F1-Score: {f1:.3f}\")\n",
    "    \n",
    "    print(\"#\"*60 + '\\n', file=log_file)\n",
    "    print(classification_report(label_test, y_pred_bool), file=log_file)\n",
    "    print(\"#\"*60+ '\\n', file=log_file)\n",
    "    \n",
    "    # Flush log file\n",
    "    log_file.flush()\n",
    "    log_file.close()\n",
    "    \n",
    "    # Save final result\n",
    "    with open(os.path.join(log_dir[:-16], 'output.txt'),'a') as f:\n",
    "        f.write('\\n\\n')\n",
    "        f.write(log_dir)\n",
    "        f.write('\\n')\n",
    "        f.write(f\"Best Test F1-Score: {f1:.3f}\")\n",
    "        \n",
    "    # save model and architecture to single file\n",
    "    if f1 > best_predefined_f1:\n",
    "        model.save(os.path.join(log_dir, \"model.h5\"))\n",
    "        print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Params\n",
    "lstm_size_list = [128, 256, 512]\n",
    "attention_heads_list = [4, 8, 12]\n",
    "dropout_list = [0.1, 0.25, 0.5]\n",
    "rec_dropout_list = [0.1, 0.25, 0.5]\n",
    "lr_list = [1e-3, 5e-4, 1e-4, 5e-5, 5e-6]\n",
    "\n",
    "all_params = [lstm_size_list] + [dropout_list] + [rec_dropout_list] + [attention_heads_list] + [lr_list]\n",
    "\n",
    "for each in itertools.product(*all_params):    \n",
    "    lstm_size, dropout, rec_dropout, attention_heads, lr = each\n",
    "    \n",
    "    # Params\n",
    "    print('lstm_size: ' + str(lstm_size))\n",
    "    print('\\tdropout: ' + str(dropout))\n",
    "    print('\\trec_dropout: ' + str(rec_dropout))\n",
    "    print('\\tattention_heads: ' + str(attention_heads))    \n",
    "    print('\\tlr: ' + str(lr))\n",
    "    \n",
    "    # train\n",
    "    train_model(input_train, input_test, label_train, label_test, lstm_size, dropout, rec_dropout, lr, epochs=50,\n",
    "                att_heads=attention_heads, max_length=max_length, vocab_size=vocab_size, embed_size=embed_size, emb_trainable=False,\n",
    "                batch=128, early_stopping=3,\n",
    "                save_dir=\"D:/Outputs_Mestrado/resultados_Clareza/checkpoins_resposta_keras_mh_att/\",\n",
    "                best_predefined_f1=0.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_env",
   "language": "python",
   "name": "keras_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
