{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow-gpu\n",
    "# Downgrade \n",
    "#smart_open to 1.10.0 -> https://github.com/RaRe-Technologies/smart_open/issues/475\n",
    "# python -m pip install -U smart_open==1.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# How to make deterministic experiments?\n",
    "###########################################\n",
    "\n",
    "# Main Sources:\n",
    "    # 1) https://github.com/NVIDIA/tensorflow-determinism\n",
    "    # 2) https://pypi.org/project/tensorflow-determinism/#description\n",
    "            # There are currently two main ways to access GPU-deterministic functionality in TensorFlow for most\n",
    "            # deep learning applications. \n",
    "            # 2.1) The first way is to use an NVIDIA NGC TensorFlow container. - https://www.nvidia.com/en-us/gpu-cloud/containers/\n",
    "            # 2.2. The second way is to use version 1.14, 1.15, or 2.0 of stock TensorFlow with GPU support, \n",
    "            #      plus the application of a patch supplied in this repo.\n",
    "\n",
    "# # # Ensure Deterministic behaviour\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "# Now using tensorflow 2.1.0, so no need to patch\n",
    "# from tfdeterminism import patch\n",
    "#patch()\n",
    "\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "from distutils.version import LooseVersion\n",
    "from tqdm import tqdm_notebook\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "import warnings\n",
    "import pickle\n",
    "import gc\n",
    "import sys\n",
    "from json import dumps\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "# Data\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Viz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow.keras.backend as K\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dropout, Bidirectional, LSTM, Flatten, Dense, Reshape, Layer, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks.callbacks import EarlyStopping\n",
    "#from keras.callbacks import EarlyStopping, TensorBoard\n",
    "#from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from keras_multi_head import MultiHead, MultiHeadAttention\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# NLP Models\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "#w2v_models_path = 'C:/Users/arthu/Desktop/22032020 - Experimentos/05. Organizado/02. Notebooks/models/'\n",
    "w2v_models_path = 'D:/03. Documentos/Mestrado/Dissertação/07 .Dissertação Final/02. Experimentos/02. Word Embbedings/w2v/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS\n",
    "# def f1_score(y_true, y_pred):\n",
    "\n",
    "#     # Count positive samples.\n",
    "#     c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "#     c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "#     c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "# #     # If there are no true samples, fix the F1 score at 0.\n",
    "# #     if c3 == 0:\n",
    "# #         return 0\n",
    "\n",
    "#     # How many selected items are relevant?\n",
    "#     precision = c1 / c2\n",
    "\n",
    "#     # How many relevant items are selected?\n",
    "#     recall = c1 / c3\n",
    "\n",
    "#     # Calculate f1_score\n",
    "#     f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "#     return f1_score\n",
    "\n",
    "# def recall(y_true, y_pred):\n",
    "\n",
    "#     # Count positive samples.\n",
    "#     c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "#     c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "# #     # If there are no true samples, fix the F1 score at 0.\n",
    "# #     if c3 == 0:\n",
    "# #         return 0\n",
    "\n",
    "#     # How many relevant items are selected?\n",
    "#     recall = c1 / c3\n",
    "    \n",
    "#     return recall\n",
    "\n",
    "\n",
    "# def precision(y_true, y_pred):\n",
    "\n",
    "#     # Count positive samples.\n",
    "#     c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "#     c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "\n",
    "#     # How many selected items are relevant?\n",
    "#     precision = c1 / c2\n",
    "\n",
    "#     return precision\n",
    "\n",
    "# https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('tf version: ' + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables\n",
    "current_exp = 'Atendimento-Unbalanced-Binary'\n",
    "if 'Binary' in current_exp:\n",
    "    binary = True\n",
    "else:\n",
    "    binary = False\n",
    "    \n",
    "base_path = 'D:/03. Documentos/Mestrado/22032020 - Experimentos/05. Organizado/03. Datasets/' + current_exp\n",
    "save_path = 'output'\n",
    "\n",
    "sentence = 'req-text'\n",
    "label = 'Atendimento'\n",
    "\n",
    "x_train_file = 'X_train.csv'\n",
    "y_train_file = 'y_train.csv'\n",
    "x_test_file = 'X_test.csv'\n",
    "y_test_file = 'y_test.csv'\n",
    "\n",
    "#Load data\n",
    "X_train = pd.read_csv(os.path.join(base_path, x_train_file), sep=';', encoding='utf-8')\n",
    "y_train = pd.read_csv(os.path.join(base_path, y_train_file), sep=';', encoding='utf-8')\n",
    "X_test = pd.read_csv(os.path.join(base_path, x_test_file), sep=';', encoding='utf-8')\n",
    "y_test = pd.read_csv(os.path.join(base_path, y_test_file), sep=';', encoding='utf-8')\n",
    "\n",
    "#Checking on data\n",
    "print(X_train.columns)\n",
    "print(X_train.shape)\n",
    "print(y_train[label].value_counts())\n",
    "print(y_test[label].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only text columns\n",
    "X_train.drop(columns=X_train.columns[2:], inplace=True)\n",
    "X_test.drop(columns=X_train.columns[2:], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "X_train['sentence'] = X_train[sentence]\n",
    "X_test['sentence'] = X_test[sentence]\n",
    "\n",
    "y_train['label'] = y_train[label]\n",
    "y_test['label'] = y_test[label]\n",
    "\n",
    "##################################################################\n",
    "# CUT DATAFRAME\n",
    "# factor = 10000\n",
    "# df = pd.concat([df[df.label=='1'][0:factor], df[df.label=='0'][0:factor]])\n",
    "##################################################################\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(X_train.shape[0]))\n",
    "\n",
    "# Report the classes balance.\n",
    "print('Classes distribuition: \\n')\n",
    "print(y_train[label].value_counts())\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "X_train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking lengths\n",
    "lengths = [X_train.sentence.apply(lambda x: len(x.split(' ')))]\n",
    "perc =[.25, .50, .75, .80, .85, .90, .91, .92, .93, .94, .95, .96, .97, .98, .99] \n",
    "lengths[0].describe(percentiles = perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w2v Model for Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v_cbow_esic_model=KeyedVectors.load(os.path.join(w2v_models_path,'word2vec_sg_hs_DetalhamentoSolicitacao_all_sentences_128.model'))\n",
    "#w2v_cbow_nilc_model=KeyedVectors.load_word2vec_format(os.path.join(w2v_models_path,'cbow_s300.txt'))\n",
    "w2v_cbow_nilc_model=KeyedVectors.load_word2vec_format(os.path.join(w2v_models_path,'skip_s300.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = w2v_cbow_nilc_model.wv.syn0\n",
    "print(pretrained_weights.shape)\n",
    "max_num_words = pretrained_weights.shape[0]\n",
    "embed_size = pretrained_weights.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = X_train['sentence'].append(X_test['sentence'])\n",
    "t_reading = Tokenizer(num_words=max_num_words)\n",
    "t_reading.fit_on_texts(texts)\n",
    "sequences_reading = t_reading.texts_to_sequences(texts)\n",
    "\n",
    "\n",
    "len_sequences = [len(seq) for seq in sequences_reading]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An \"interface\" to matplotlib.axes.Axes.hist() method\n",
    "n, bins, patches = plt.hist(x=len_sequences, bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Sentence Lenght Distribution')\n",
    "maxfreq = n.max()\n",
    "# Set a clean upper y-axis limit.\n",
    "plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc = 384\n",
    "len_sequences_truncated = [trunc if seq>=trunc else seq for seq in len_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An \"interface\" to matplotlib.axes.Axes.hist() method\n",
    "n, bins, patches = plt.hist(x=len_sequences_truncated, bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Sentence Lenght Distribution')\n",
    "maxfreq = n.max()\n",
    "# Set a clean upper y-axis limit.\n",
    "plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=384\n",
    "\n",
    "# Define tokenizer and fit train data\n",
    "t = Tokenizer(num_words=max_num_words)\n",
    "t.fit_on_texts(X_train['sentence'].append(X_test['sentence']))\n",
    "word_index = t.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "    \n",
    "def get_seqs(text):    \n",
    "    sequences = t.texts_to_sequences(text)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y_train)\n",
    "    y_train_enc = le.transform(y_train)\n",
    "    y_test_enc = le.transform(y_test)\n",
    "    #if binary:\n",
    "    #    return y_train_enc, y_test_enc\n",
    "    #else:\n",
    "    return pd.get_dummies(y_train_enc), pd.get_dummies(y_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and Y\n",
    "label_train, label_test = prepare_targets(y_train.label.values, y_test.label.values)\n",
    "num_labels = len(set(label_train))\n",
    "input_train = get_seqs(X_train.sentence)\n",
    "input_test = get_seqs(X_test.sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, embed_size))\n",
    "for word, i in t.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = w2v_cbow_nilc_model.wv.__getitem__(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    # Words not in vocab -> Frequency less than 5 word\n",
    "    except KeyError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/\n",
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
    "        super(attention, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
    "        at=K.softmax(et)\n",
    "        at=K.expand_dims(at,axis=-1)\n",
    "        output=x*at\n",
    "        return K.sum(output,axis=1)\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(attention,self).get_config()\n",
    "\n",
    "# class MultiHeadSelfAttention(Layer):\n",
    "#     def __init__(self, embed_dim, num_heads=8):\n",
    "#         super(MultiHeadSelfAttention, self).__init__()\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.num_heads = num_heads\n",
    "#         if embed_dim % num_heads != 0:\n",
    "#             raise ValueError(\n",
    "#                 f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "#             )\n",
    "#         self.projection_dim = embed_dim // num_heads\n",
    "#         self.query_dense = Dense(embed_dim)\n",
    "#         self.key_dense = Dense(embed_dim)\n",
    "#         self.value_dense = Dense(embed_dim)\n",
    "#         self.combine_heads = Dense(embed_dim)\n",
    "\n",
    "#     def attention(self, query, key, value):\n",
    "#         score = tf.matmul(query, key, transpose_b=True)\n",
    "#         dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "#         scaled_score = score / tf.math.sqrt(dim_key)\n",
    "#         weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "#         output = tf.matmul(weights, value)\n",
    "#         return output, weights\n",
    "\n",
    "#     def separate_heads(self, x, batch_size):\n",
    "#         x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "#         return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "#         batch_size = tf.shape(inputs)[0]\n",
    "#         query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "#         key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "#         value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "#         query = self.separate_heads(\n",
    "#             query, batch_size\n",
    "#         )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "#         key = self.separate_heads(\n",
    "#             key, batch_size\n",
    "#         )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "#         value = self.separate_heads(\n",
    "#             value, batch_size\n",
    "#         )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "#         attention, weights = self.attention(query, key, value)\n",
    "#         attention = tf.transpose(\n",
    "#             attention, perm=[0, 2, 1, 3]\n",
    "#         )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "#         concat_attention = tf.reshape(\n",
    "#             attention, (batch_size, -1, self.embed_dim)\n",
    "#         )  # (batch_size, seq_len, embed_dim)\n",
    "#         output = self.combine_heads(\n",
    "#             concat_attention\n",
    "#         )  # (batch_size, seq_len, embed_dim)\n",
    "#         return output\n",
    "    \n",
    "# class TokenAndPositionEmbedding(Layer):\n",
    "#     def __init__(self, maxlen, vocab_size, emded_dim):\n",
    "#         super(TokenAndPositionEmbedding, self).__init__()\n",
    "#         self.token_emb = Embedding(input_dim=vocab_size, output_dim=emded_dim)\n",
    "#         self.pos_emb = Embedding(input_dim=maxlen, output_dim=emded_dim)\n",
    "\n",
    "#     def call(self, x):\n",
    "#         maxlen = tf.shape(x)[-1]\n",
    "#         positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "#         positions = self.pos_emb(positions)\n",
    "#         x = self.token_emb(x)\n",
    "#         return x + positions\n",
    "    \n",
    "# class TransformerBlock(Layer):\n",
    "#     def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "#         super(TransformerBlock, self).__init__()\n",
    "#         self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "#         self.ffn = Sequential(\n",
    "#             [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim),]\n",
    "#         )\n",
    "#         self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "#         self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "#         self.dropout1 = Dropout(rate)\n",
    "#         self.dropout2 = Dropout(rate)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         attn_output = self.att(inputs)\n",
    "#         attn_output = self.dropout1(attn_output, training=False)\n",
    "#         out1 = self.layernorm1(inputs + attn_output)\n",
    "#         ffn_output = self.ffn(out1)\n",
    "#         ffn_output = self.dropout2(ffn_output, training=False)\n",
    "#         return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_dim = 32  # Embedding size for each token\n",
    "# num_heads = 2  # Number of attention heads\n",
    "# ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "# inputs = Input(shape=(max_length,))\n",
    "# embedding_layer = TokenAndPositionEmbedding(max_length, vocab_size, embed_dim)\n",
    "# x = embedding_layer(inputs)\n",
    "# transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "# x = transformer_block(x)\n",
    "# x = GlobalAveragePooling1D()(x)\n",
    "# x = Dropout(0.1)(x)\n",
    "# x = Dense(20, activation=\"relu\")(x)\n",
    "# x = Dropout(0.1)(x)\n",
    "# outputs = Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "# model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention\n",
    "#https://keras.io/examples/nlp/text_classification_with_transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "def train_model(input_train, input_test, label_train, label_test,\n",
    "                lstm_size=128, dropout=0.2, rec_dropout=0.2, lr=0.005, epochs=50, att_heads=4, max_length=128, \n",
    "                vocab_size=None, embed_size=None, emb_trainable=False, batch=128, early_stopping=5,\n",
    "                save_dir=\"D:/resultados/checkpoins_solicitacao_keras_mh_att/\", best_predefined_f1=0.390):\n",
    "\n",
    "    # Time now\n",
    "    now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Log\n",
    "    log_dir = save_dir + now\n",
    "    \n",
    "    # Model Saver    \n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    \n",
    "    log_file = open(os.path.join(log_dir,\"log.txt\"), mode=\"a\")\n",
    "    \n",
    "    # Save Params\n",
    "    params = {\n",
    "        'lstm_size': lstm_size,\n",
    "        'dropout': dropout,\n",
    "        'rec_dropout': rec_dropout,\n",
    "        'lr': lr,\n",
    "        'epochs': epochs,\n",
    "        'att_heads': att_heads,\n",
    "        'max_length': max_length,\n",
    "        'vocab_size': vocab_size,\n",
    "        'embed_size': embed_size,\n",
    "        'emb_trainable': emb_trainable,\n",
    "        'batch': batch,\n",
    "        'early_stopping': early_stopping,\n",
    "        'log_dir': log_dir,\n",
    "        'best_predefined_f1': best_predefined_f1}\n",
    "    \n",
    "    # Saving Parameters\n",
    "    with open(os.path.join(log_dir, 'params.txt'),'a') as f:\n",
    "        f.write('\\n\\n' + ('#'*60))\n",
    "        f.write('\\nParameters:\\n')\n",
    "        f.write('now: ' + str(now))\n",
    "        f.write('\\n' + dumps(params) + '\\n')\n",
    "    \n",
    "    # input\n",
    "    inp = Input(shape=(max_length, ))\n",
    "\n",
    "    # Embedding layer - https://keras.io/layers/embeddings/\n",
    "    embedding_layer = Embedding(vocab_size,\n",
    "                                embed_size,                                \n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_length,\n",
    "                                trainable=emb_trainable,\n",
    "                                name='Embedding')(inp)\n",
    "\n",
    "    # Bidirectional Layer\n",
    "    bilstm_layer = Bidirectional(LSTM(\n",
    "                        units=lstm_size,\n",
    "                        return_sequences=True,\n",
    "                        dropout=dropout,\n",
    "                        recurrent_dropout=rec_dropout,\n",
    "                        name='LSTM'))(embedding_layer)\n",
    "    \n",
    "    bilstm_layer_2 = Bidirectional(LSTM(\n",
    "                        units=lstm_size,\n",
    "                        return_sequences=True,\n",
    "                        dropout=dropout,\n",
    "                        recurrent_dropout=rec_dropout,\n",
    "                        name='LSTM'))(bilstm_layer)\n",
    "\n",
    "    # MultiHead-Attention Layer\n",
    "    #https://pypi.org/project/keras-multi-head/\n",
    "    #multiHead_att_layer = MultiHeadAttention(head_num=att_heads, name='Multi-Head-Attention')(bilstm_layer)\n",
    "    if att_heads:\n",
    "        #att_layer = MultiHeadSelfAttention(embed_size, att_heads)(bilstm_layer)\n",
    "        att_layer = MultiHeadAttention(head_num=att_heads, name='Multi-Head-Attention')(bilstm_layer)\n",
    "    else:\n",
    "        att_layer = attention()(bilstm_layer)\n",
    "\n",
    "    dropout_layer = Dropout(0.1)(bilstm_layer_2)\n",
    "\n",
    "    # # Flatten\n",
    "    flatten_layer = Flatten(name='Flatten')(dropout_layer)\n",
    "\n",
    "    #dense_intermed_layer = Dense(128, activation='relu')(flatten_layer)\n",
    "    #dropout_intermed_2_layer = Dropout(dropout)(dense_intermed_layer)\n",
    "\n",
    "    # # # # Dense Layer\n",
    "    #if binary:\n",
    "    #    dense_layer = Dense(1, activation='sigmoid')(flatten_layer)    \n",
    "    #else:\n",
    "    dense_layer = Dense(num_labels, activation='softmax')(flatten_layer)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=dense_layer)\n",
    "    # model.summary()\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(optimizer=Adam(lr=lr), loss='binary_crossentropy', metrics=['accuracy', precision_m, recall_m, f1_m])\n",
    "    \n",
    "    # callbacks\n",
    "    es_callback = EarlyStopping(monitor='val_loss', patience=early_stopping, verbose=1, mode='min')\n",
    "    \n",
    "    # Fitting Model\n",
    "    model.fit(input_train,\n",
    "              label_train,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch,\n",
    "              validation_data=(input_test, label_test),\n",
    "              verbose=0,\n",
    "              callbacks=[es_callback])\n",
    "    \n",
    "    # PLOT LOSS\n",
    "    plt.title('Loss')\n",
    "    plt.plot(model.history.history['loss'], label='train')\n",
    "    plt.plot(model.history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    #plt.show();\n",
    "    plt.savefig(os.path.join(log_dir,'Loss.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Classification\n",
    "    y_pred = model.predict(input_test, batch_size=batch, verbose=1)\n",
    "    y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    #if not binary:\n",
    "    label_test = np.argmax(label_test.values, axis=1)\n",
    "    \n",
    "    # Metrics\n",
    "    f1 = f1_score(label_test, y_pred_bool, average='weighted')\n",
    "    print(f\"Best Test F1-Score: {f1:.3f}\")\n",
    "    \n",
    "    print(\"#\"*60 + '\\n', file=log_file)\n",
    "    print(classification_report(label_test, y_pred_bool), file=log_file)\n",
    "    print(\"#\"*60+ '\\n', file=log_file)\n",
    "    \n",
    "    # Flush log file\n",
    "    log_file.flush()\n",
    "    log_file.close()\n",
    "    \n",
    "    # Save final result\n",
    "    with open(os.path.join(log_dir[:-16], 'output.txt'),'a') as f:\n",
    "        f.write('\\n\\n')\n",
    "        f.write(log_dir)\n",
    "        f.write('\\n')\n",
    "        f.write(f\"Best Test F1-Score: {f1:.3f}\")\n",
    "        \n",
    "    # save model and architecture to single file\n",
    "    if f1 > best_predefined_f1:\n",
    "        model.save(os.path.join(log_dir, \"model.h5\"))\n",
    "        print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Execution\n",
    "\n",
    "lstm_size, dropout, rec_dropout, attention_heads, lr = [128, 0.1, 0.1, 2, 5e-05]\n",
    "batch_size = 64\n",
    "train_model(input_train, input_test, label_train, label_test, lstm_size, dropout, rec_dropout, lr, epochs=50,\n",
    "                 att_heads=attention_heads, max_length=max_length, vocab_size=vocab_size, embed_size=embed_size, emb_trainable=False,\n",
    "                 batch=batch_size, early_stopping=3,\n",
    "                 save_dir=\"D:/Outputs_Mestrado/resultados_Atendimento/checkpoins_solicitacao_binary_keras_mh_att/\",\n",
    "                 best_predefined_f1=0.68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Params\n",
    "#lstm_size_list = [128, 256, 512]\n",
    "lstm_size_list = [512, 1024, 2048]\n",
    "#attention_heads_list = [2, 4, 8]\n",
    "#dropout_list = [0.1, 0.25, 0.5]\n",
    "#rec_dropout_list = [0.1, 0.25, 0.5]\n",
    "dropout_list = [0.1]\n",
    "rec_dropout_list = [0.1]\n",
    "#lr_list = [1e-3, 5e-4, 1e-4, 5e-5, 5e-6]\n",
    "lr_list = [5e-4, 1e-4, 5e-5, 5e-6]\n",
    "\n",
    "#all_params = [lstm_size_list] + [dropout_list] + [rec_dropout_list] + [attention_heads_list] + [lr_list]\n",
    "all_params = [lstm_size_list] + [dropout_list] + [rec_dropout_list] + [lr_list]\n",
    "\n",
    "for each in itertools.product(*all_params):    \n",
    "    #lstm_size, dropout, rec_dropout, attention_heads, lr = each\n",
    "    lstm_size, dropout, rec_dropout, lr = each\n",
    "        \n",
    "    \n",
    "    #if attention_heads==8 or lstm_size==256:\n",
    "    #    batch_size = 64\n",
    "    #elif attention_heads> 8 or lstm_size>256:\n",
    "    #    batch_size = 16\n",
    "    #else:\n",
    "    #    batch_size = 256\n",
    "    batch_size=32\n",
    "        \n",
    "    attention_heads = False\n",
    "    \n",
    "    # Params\n",
    "    print('lstm_size: ' + str(lstm_size))\n",
    "    print('\\tdropout: ' + str(dropout))\n",
    "    print('\\trec_dropout: ' + str(rec_dropout))\n",
    "    #print('\\tattention_heads: ' + str(attention_heads))    \n",
    "    print('\\tlr: ' + str(lr))    \n",
    "    \n",
    "    # train\n",
    "    train_model(input_train, input_test, label_train, label_test, lstm_size, dropout, rec_dropout, lr, epochs=50,\n",
    "                 att_heads=attention_heads, max_length=max_length, vocab_size=vocab_size, embed_size=embed_size, emb_trainable=False,\n",
    "                 batch=batch_size, early_stopping=3,\n",
    "                 save_dir=\"D:/Outputs_Mestrado/resultados_Atendimento/checkpoins_solicitacao_binary_keras_mh_att/\",\n",
    "                 best_predefined_f1=0.68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_env",
   "language": "python",
   "name": "keras_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
