{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow-gpu\n",
    "# Downgrade \n",
    "#smart_open to 1.10.0 -> https://github.com/RaRe-Technologies/smart_open/issues/475\n",
    "# python -m pip install -U smart_open==1.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# How to make deterministic experiments?\n",
    "###########################################\n",
    "\n",
    "# Main Sources:\n",
    "    # 1) https://github.com/NVIDIA/tensorflow-determinism\n",
    "    # 2) https://pypi.org/project/tensorflow-determinism/#description\n",
    "            # There are currently two main ways to access GPU-deterministic functionality in TensorFlow for most\n",
    "            # deep learning applications. \n",
    "            # 2.1) The first way is to use an NVIDIA NGC TensorFlow container. - https://www.nvidia.com/en-us/gpu-cloud/containers/\n",
    "            # 2.2. The second way is to use version 1.14, 1.15, or 2.0 of stock TensorFlow with GPU support, \n",
    "            #      plus the application of a patch supplied in this repo.\n",
    "\n",
    "# # # Ensure Deterministic behaviour\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "# Now using tensorflow 2.1.0, so no need to patch\n",
    "# from tfdeterminism import patch\n",
    "#patch()\n",
    "\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# utils\n",
    "from distutils.version import LooseVersion\n",
    "from tqdm import tqdm_notebook\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "import warnings\n",
    "import pickle\n",
    "import gc\n",
    "import sys\n",
    "from json import dumps\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "# Data\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Viz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow.keras.backend as K\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dropout, Bidirectional, LSTM, Flatten, Dense, Reshape\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks.callbacks import EarlyStopping\n",
    "#from keras.callbacks import EarlyStopping, TensorBoard\n",
    "#from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from keras_multi_head import MultiHead, MultiHeadAttention\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# NLP Models\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "#w2v_models_path = 'C:/Users/arthu/Desktop/22032020 - Experimentos/05. Organizado/02. Notebooks/models/'\n",
    "w2v_models_path = 'D:/Mestrado/Dissertação/07 .Dissertação Final/02. Experimentos/02. Word Embbedings/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS\n",
    "# def f1_score(y_true, y_pred):\n",
    "\n",
    "#     # Count positive samples.\n",
    "#     c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "#     c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "#     c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "# #     # If there are no true samples, fix the F1 score at 0.\n",
    "# #     if c3 == 0:\n",
    "# #         return 0\n",
    "\n",
    "#     # How many selected items are relevant?\n",
    "#     precision = c1 / c2\n",
    "\n",
    "#     # How many relevant items are selected?\n",
    "#     recall = c1 / c3\n",
    "\n",
    "#     # Calculate f1_score\n",
    "#     f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "#     return f1_score\n",
    "\n",
    "# def recall(y_true, y_pred):\n",
    "\n",
    "#     # Count positive samples.\n",
    "#     c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "#     c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "# #     # If there are no true samples, fix the F1 score at 0.\n",
    "# #     if c3 == 0:\n",
    "# #         return 0\n",
    "\n",
    "#     # How many relevant items are selected?\n",
    "#     recall = c1 / c3\n",
    "    \n",
    "#     return recall\n",
    "\n",
    "\n",
    "# def precision(y_true, y_pred):\n",
    "\n",
    "#     # Count positive samples.\n",
    "#     c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "#     c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "\n",
    "#     # How many selected items are relevant?\n",
    "#     precision = c1 / c2\n",
    "\n",
    "#     return precision\n",
    "\n",
    "# https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf version: 2.1.0-rc2\n"
     ]
    }
   ],
   "source": [
    "print('tf version: ' + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['pid', 'req-text', 'resp-text', '1funct-request', '2pronoun-request',\n",
      "       '3ppron-request', '4i-request', '5we-request', '6you-request',\n",
      "       '7shehe-request',\n",
      "       ...\n",
      "       '58home-response', '59money-response', '60relig-response',\n",
      "       '61death-response', '62assent-response', '63nonfl-response',\n",
      "       '64filler-response', 'Clareza', 'Atendimento', 'tempo_resposta'],\n",
      "      dtype='object', length=134)\n",
      "(31586, 134)\n",
      "2    21682\n",
      "0     9904\n",
      "Name: Atendimento, dtype: int64\n",
      "2    9338\n",
      "0    4200\n",
      "Name: Atendimento, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Variables\n",
    "current_exp = 'Atendimento-Unbalanced-Binary'\n",
    "if 'Binary' in current_exp:\n",
    "    binary = True\n",
    "else:\n",
    "    binary = False\n",
    "    \n",
    "base_path = 'C:/Users/arthu/Desktop/22032020 - Experimentos/05. Organizado/03. Datasets/' + current_exp\n",
    "save_path = 'output'\n",
    "\n",
    "sentence = 'resp-text'\n",
    "label = 'Atendimento'\n",
    "\n",
    "x_train_file = 'X_train.csv'\n",
    "y_train_file = 'y_train.csv'\n",
    "x_test_file = 'X_test.csv'\n",
    "y_test_file = 'y_test.csv'\n",
    "\n",
    "#Load data\n",
    "X_train = pd.read_csv(os.path.join(base_path, x_train_file), sep=';', encoding='utf-8')\n",
    "y_train = pd.read_csv(os.path.join(base_path, y_train_file), sep=';', encoding='utf-8')\n",
    "X_test = pd.read_csv(os.path.join(base_path, x_test_file), sep=';', encoding='utf-8')\n",
    "y_test = pd.read_csv(os.path.join(base_path, y_test_file), sep=';', encoding='utf-8')\n",
    "\n",
    "#Checking on data\n",
    "print(X_train.columns)\n",
    "print(X_train.shape)\n",
    "print(y_train[label].value_counts())\n",
    "print(y_test[label].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only text columns\n",
    "X_train.drop(columns=X_train.columns[3:], inplace=True)\n",
    "X_test.drop(columns=X_train.columns[3:], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 31,586\n",
      "\n",
      "Classes distribuition: \n",
      "\n",
      "2    21682\n",
      "0     9904\n",
      "Name: Atendimento, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>req-text</th>\n",
       "      <th>resp-text</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27971</th>\n",
       "      <td>519005</td>\n",
       "      <td>Prezados, solicito informações sobre o FNO des...</td>\n",
       "      <td>Prezado Senhor : Obrigado por utilizar esse ca...</td>\n",
       "      <td>Prezado Senhor : Obrigado por utilizar esse ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4148</th>\n",
       "      <td>557020</td>\n",
       "      <td>Quantos servidores ocupantes do cargo de Deleg...</td>\n",
       "      <td>Caro Requerente , Em resposta ao seu pedido de...</td>\n",
       "      <td>Caro Requerente , Em resposta ao seu pedido de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2527</th>\n",
       "      <td>367364</td>\n",
       "      <td>Há código de vaga para Psicólogo no HU/UFGD?</td>\n",
       "      <td>Encaminhamos vossa solicitação para a Divisão...</td>\n",
       "      <td>Encaminhamos vossa solicitação para a Divisão...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29114</th>\n",
       "      <td>452095</td>\n",
       "      <td>Prezados, gostaria de esclarecimentos sobre a ...</td>\n",
       "      <td>Prezado Sr . Edson , Em resposta a sua solicit...</td>\n",
       "      <td>Prezado Sr . Edson , Em resposta a sua solicit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>489220</td>\n",
       "      <td>No site dados.gov.br têm sido disponibilizados...</td>\n",
       "      <td>Senhora Carolina, O Serviço de Informações ao ...</td>\n",
       "      <td>Senhora Carolina, O Serviço de Informações ao ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22896</th>\n",
       "      <td>555233</td>\n",
       "      <td>O Requerente, AFT/MTE aposentado, SIAPE 025372...</td>\n",
       "      <td>Prezado a Senhor a , Em resposta ao seu pedido...</td>\n",
       "      <td>Prezado a Senhor a , Em resposta ao seu pedido...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23246</th>\n",
       "      <td>405393</td>\n",
       "      <td>Fiz um pedido anterior de informações sobre co...</td>\n",
       "      <td>Prezado , Em resposta ao seu pedido informamos...</td>\n",
       "      <td>Prezado , Em resposta ao seu pedido informamos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16050</th>\n",
       "      <td>468913</td>\n",
       "      <td>Observação: As perguntas a seguir, ficam mais ...</td>\n",
       "      <td>Prezado , Segue resposta . Informamos que cas...</td>\n",
       "      <td>Prezado , Segue resposta . Informamos que cas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6778</th>\n",
       "      <td>441341</td>\n",
       "      <td>Gostaria de saber a quantidade de códigos de v...</td>\n",
       "      <td>Prezado , Informo que não há código de vaga di...</td>\n",
       "      <td>Prezado , Informo que não há código de vaga di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>512226</td>\n",
       "      <td>Solicito da CAIXA ECONÔMICA FEDERAL cópia do p...</td>\n",
       "      <td>Prezado a Cidadão ã , 1 . Conforme solicitação...</td>\n",
       "      <td>Prezado a Cidadão ã , 1 . Conforme solicitação...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pid                                           req-text  \\\n",
       "27971  519005  Prezados, solicito informações sobre o FNO des...   \n",
       "4148   557020  Quantos servidores ocupantes do cargo de Deleg...   \n",
       "2527   367364       Há código de vaga para Psicólogo no HU/UFGD?   \n",
       "29114  452095  Prezados, gostaria de esclarecimentos sobre a ...   \n",
       "2388   489220  No site dados.gov.br têm sido disponibilizados...   \n",
       "22896  555233  O Requerente, AFT/MTE aposentado, SIAPE 025372...   \n",
       "23246  405393  Fiz um pedido anterior de informações sobre co...   \n",
       "16050  468913  Observação: As perguntas a seguir, ficam mais ...   \n",
       "6778   441341  Gostaria de saber a quantidade de códigos de v...   \n",
       "1723   512226  Solicito da CAIXA ECONÔMICA FEDERAL cópia do p...   \n",
       "\n",
       "                                               resp-text  \\\n",
       "27971  Prezado Senhor : Obrigado por utilizar esse ca...   \n",
       "4148   Caro Requerente , Em resposta ao seu pedido de...   \n",
       "2527    Encaminhamos vossa solicitação para a Divisão...   \n",
       "29114  Prezado Sr . Edson , Em resposta a sua solicit...   \n",
       "2388   Senhora Carolina, O Serviço de Informações ao ...   \n",
       "22896  Prezado a Senhor a , Em resposta ao seu pedido...   \n",
       "23246  Prezado , Em resposta ao seu pedido informamos...   \n",
       "16050   Prezado , Segue resposta . Informamos que cas...   \n",
       "6778   Prezado , Informo que não há código de vaga di...   \n",
       "1723   Prezado a Cidadão ã , 1 . Conforme solicitação...   \n",
       "\n",
       "                                                sentence  \n",
       "27971  Prezado Senhor : Obrigado por utilizar esse ca...  \n",
       "4148   Caro Requerente , Em resposta ao seu pedido de...  \n",
       "2527    Encaminhamos vossa solicitação para a Divisão...  \n",
       "29114  Prezado Sr . Edson , Em resposta a sua solicit...  \n",
       "2388   Senhora Carolina, O Serviço de Informações ao ...  \n",
       "22896  Prezado a Senhor a , Em resposta ao seu pedido...  \n",
       "23246  Prezado , Em resposta ao seu pedido informamos...  \n",
       "16050   Prezado , Segue resposta . Informamos que cas...  \n",
       "6778   Prezado , Informo que não há código de vaga di...  \n",
       "1723   Prezado a Cidadão ã , 1 . Conforme solicitação...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################################################\n",
    "X_train['sentence'] = X_train[sentence]\n",
    "X_test['sentence'] = X_test[sentence]\n",
    "\n",
    "y_train['label'] = y_train[label]\n",
    "y_test['label'] = y_test[label]\n",
    "\n",
    "##################################################################\n",
    "# CUT DATAFRAME\n",
    "# factor = 10000\n",
    "# df = pd.concat([df[df.label=='1'][0:factor], df[df.label=='0'][0:factor]])\n",
    "##################################################################\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(X_train.shape[0]))\n",
    "\n",
    "# Report the classes balance.\n",
    "print('Classes distribuition: \\n')\n",
    "print(y_train[label].value_counts())\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "X_train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    31586.000000\n",
       "mean       158.355379\n",
       "std        164.789252\n",
       "min          1.000000\n",
       "25%         52.000000\n",
       "50%        110.000000\n",
       "75%        207.000000\n",
       "80%        239.000000\n",
       "85%        279.000000\n",
       "90%        334.000000\n",
       "91%        352.000000\n",
       "92%        371.000000\n",
       "93%        395.000000\n",
       "94%        425.000000\n",
       "95%        453.750000\n",
       "96%        493.000000\n",
       "97%        549.000000\n",
       "98%        670.000000\n",
       "99%        862.000000\n",
       "max       1885.000000\n",
       "Name: sentence, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking lengths\n",
    "lengths = [X_train.sentence.apply(lambda x: len(x.split(' ')))]\n",
    "perc =[.25, .50, .75, .80, .85, .90, .91, .92, .93, .94, .95, .96, .97, .98, .99] \n",
    "lengths[0].describe(percentiles = perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w2v Model for Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-e5416c8427c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#w2v_cbow_esic_model=KeyedVectors.load(os.path.join(w2v_models_path,'word2vec_sg_hs_DetalhamentoSolicitacao_all_sentences_128.model'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mw2v_cbow_nilc_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v_models_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'cbow_s300.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1496\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1497\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1498\u001b[1;33m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[0;32m   1499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mline_no\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m                 \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mb''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unexpected end of input; is count incorrect or file otherwise damaged?\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#w2v_cbow_esic_model=KeyedVectors.load(os.path.join(w2v_models_path,'word2vec_sg_hs_DetalhamentoSolicitacao_all_sentences_128.model'))\n",
    "w2v_cbow_nilc_model=KeyedVectors.load_word2vec_format(os.path.join(w2v_models_path,'cbow_s300.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = w2v_cbow_nilc_model.wv.syn0\n",
    "print(pretrained_weights.shape)\n",
    "max_num_words = pretrained_weights.shape[0]\n",
    "embed_size = pretrained_weights.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=128\n",
    "\n",
    "# Define tokenizer and fit train data\n",
    "t = Tokenizer(num_words=max_num_words)\n",
    "t.fit_on_texts(X_train['sentence'].append(X_test['sentence']))\n",
    "word_index = t.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "    \n",
    "def get_seqs(text):    \n",
    "    sequences = t.texts_to_sequences(text)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y_train)\n",
    "    y_train_enc = le.transform(y_train)\n",
    "    y_test_enc = le.transform(y_test)\n",
    "    #if binary:\n",
    "    #    return y_train_enc, y_test_enc\n",
    "    #else:\n",
    "    return pd.get_dummies(y_train_enc), pd.get_dummies(y_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and Y\n",
    "label_train, label_test = prepare_targets(y_train.label.values, y_test.label.values)\n",
    "num_labels = len(set(label_train))\n",
    "input_train = get_seqs(X_train.sentence)\n",
    "input_test = get_seqs(X_test.sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, embed_size))\n",
    "for word, i in t.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = w2v_cbow_nilc_model.wv.__getitem__(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    # Words not in vocab -> Frequency less than 5 word\n",
    "    except KeyError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "def train_model(input_train, input_test, label_train, label_test,\n",
    "                lstm_size=128, dropout=0.2, rec_dropout=0.2, lr=0.005, epochs=50, att_heads=4, max_length=128, \n",
    "                vocab_size=None, embed_size=None, emb_trainable=False, batch=128, early_stopping=5,\n",
    "                save_dir=\"D:/resultados/checkpoins_solicitacao_keras_mh_att/\", best_predefined_f1=0.390):\n",
    "\n",
    "    # Time now\n",
    "    now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Log\n",
    "    log_dir = save_dir + now\n",
    "    \n",
    "    # Model Saver    \n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    \n",
    "    log_file = open(os.path.join(log_dir,\"log.txt\"), mode=\"a\")\n",
    "    \n",
    "    # Save Params\n",
    "    params = {\n",
    "        'lstm_size': lstm_size,\n",
    "        'dropout': dropout,\n",
    "        'rec_dropout': rec_dropout,\n",
    "        'lr': lr,\n",
    "        'epochs': epochs,\n",
    "        'att_heads': att_heads,\n",
    "        'max_length': max_length,\n",
    "        'vocab_size': vocab_size,\n",
    "        'embed_size': embed_size,\n",
    "        'emb_trainable': emb_trainable,\n",
    "        'batch': batch,\n",
    "        'early_stopping': early_stopping,\n",
    "        'log_dir': log_dir,\n",
    "        'best_predefined_f1': best_predefined_f1}\n",
    "    \n",
    "    # Saving Parameters\n",
    "    with open(os.path.join(log_dir, 'params.txt'),'a') as f:\n",
    "        f.write('\\n\\n' + ('#'*60))\n",
    "        f.write('\\nParameters:\\n')\n",
    "        f.write('now: ' + str(now))\n",
    "        f.write('\\n' + dumps(params) + '\\n')\n",
    "    \n",
    "    # input\n",
    "    inp = Input(shape=(max_length, ))\n",
    "\n",
    "    # Embedding layer - https://keras.io/layers/embeddings/\n",
    "    embedding_layer = Embedding(vocab_size,\n",
    "                                embed_size,                                \n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_length,\n",
    "                                trainable=emb_trainable,\n",
    "                                name='Embedding')(inp)\n",
    "\n",
    "    # Bidirectional Layer\n",
    "    bilstm_layer = Bidirectional(LSTM(\n",
    "                        units=lstm_size,\n",
    "                        return_sequences=True,\n",
    "                        dropout=dropout,\n",
    "                        recurrent_dropout=rec_dropout,\n",
    "                        name='LSTM'))(embedding_layer)    \n",
    "\n",
    "    # MultiHead-Attention Layer\n",
    "    #https://pypi.org/project/keras-multi-head/\n",
    "    multiHead_att_layer = MultiHeadAttention(head_num=att_heads, name='Multi-Head-Attention')(bilstm_layer)\n",
    "\n",
    "    dropout_intermed_layer = Dropout(0.5)(multiHead_att_layer)\n",
    "\n",
    "    # # Flatten\n",
    "    flatten_layer = Flatten(name='Flatten')(dropout_intermed_layer)\n",
    "\n",
    "    dense_intermed_layer = Dense(128, activation='relu')(flatten_layer)\n",
    "    dropout_intermed_2_layer = Dropout(0.5)(dense_intermed_layer)\n",
    "\n",
    "    # # # # Dense Layer\n",
    "    #if binary:\n",
    "    #    dense_layer = Dense(1, activation='sigmoid')(flatten_layer)    \n",
    "    #else:\n",
    "    dense_layer = Dense(num_labels, activation='softmax')(dropout_intermed_2_layer)    \n",
    "    \n",
    "    model = Model(inputs=inp, outputs=dense_layer)\n",
    "    # model.summary()\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(optimizer=Adam(lr=lr), loss='binary_crossentropy', metrics=['accuracy', precision_m, recall_m, f1_m])\n",
    "    \n",
    "    # callbacks\n",
    "    es_callback = EarlyStopping(monitor='val_loss', patience=early_stopping, verbose=1, mode='min')\n",
    "    \n",
    "    # class weights\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(label_train), np.argmax(label_train.values, axis=1))\n",
    "\n",
    "    #https://www.tensorflow.org/tutorials/structured_data/imbalanced_data\n",
    "\n",
    "    # Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "    # The sum of the weights of all examples stays the same.\n",
    "    #weight_for_0 = (1 / (len(label_train)-sum(label_train)))*(len(label_train))/2.0 \n",
    "    #weight_for_1 = (1 / sum(label_train))*(len(label_train))/2.0\n",
    "\n",
    "    #class_weights = {0: weight_for_0, 1: weight_for_1}\n",
    "    \n",
    "    # Fitting Model\n",
    "    model.fit(input_train,\n",
    "              label_train,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch,\n",
    "              validation_data=(input_test, label_test),\n",
    "              verbose=0,\n",
    "              class_weight=class_weights,\n",
    "              callbacks=[es_callback])\n",
    "    \n",
    "    # PLOT LOSS\n",
    "    plt.title('Loss')\n",
    "    plt.plot(model.history.history['loss'], label='train')\n",
    "    plt.plot(model.history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    #plt.show();\n",
    "    plt.savefig(os.path.join(log_dir,'Loss.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Classification\n",
    "    y_pred = model.predict(input_test, batch_size=batch, verbose=1)\n",
    "    y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    #if not binary:\n",
    "    label_test = np.argmax(label_test.values, axis=1)\n",
    "    \n",
    "    # Metrics\n",
    "    f1 = f1_score(label_test, y_pred_bool, average='weighted')\n",
    "    print(f\"Best Test F1-Score: {f1:.3f}\")\n",
    "    \n",
    "    print(\"#\"*60 + '\\n', file=log_file)\n",
    "    print(classification_report(label_test, y_pred_bool), file=log_file)\n",
    "    print(\"#\"*60+ '\\n', file=log_file)\n",
    "    \n",
    "    # Flush log file\n",
    "    log_file.flush()\n",
    "    log_file.close()\n",
    "    \n",
    "    # Save final result\n",
    "    with open(os.path.join(log_dir[:-16], 'output.txt'),'a') as f:\n",
    "        f.write('\\n\\n')\n",
    "        f.write(log_dir)\n",
    "        f.write('\\n')\n",
    "        f.write(f\"Best Test F1-Score: {f1:.3f}\")\n",
    "        \n",
    "    # save model and architecture to single file\n",
    "    if f1 > best_predefined_f1:\n",
    "        model.save(os.path.join(log_dir, \"model.h5\"))\n",
    "        print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Params\n",
    "lstm_size_list = [256, 512]\n",
    "dropout_list = [0.5]\n",
    "rec_dropout_list = [0.5]\n",
    "lr_list = [1e-3, 5e-4, 1e-4, 5e-5, 5e-6]\n",
    "\n",
    "all_params = [lstm_size_list] + [dropout_list] + [rec_dropout_list] + [lr_list]\n",
    "\n",
    "for each in itertools.product(*all_params):    \n",
    "    lstm_size, dropout, rec_dropout, lr = each\n",
    "    \n",
    "    # Params\n",
    "    print('lstm_size: ' + str(lstm_size))\n",
    "    print('\\tdropout: ' + str(dropout))\n",
    "    print('\\trec_dropout: ' + str(rec_dropout))\n",
    "    print('\\tlr: ' + str(lr))\n",
    "    \n",
    "    # train\n",
    "    train_model(input_train, input_test, label_train, label_test, lstm_size, dropout, rec_dropout, lr, epochs=50,\n",
    "                att_heads=4, max_length=max_length, vocab_size=vocab_size, embed_size=embed_size, emb_trainable=False, \n",
    "                batch=128, early_stopping=5,\n",
    "                save_dir=\"D:/Outputs_Mestrado/resultados_Atendimento/checkpoints_resposta_keras_BiLSTM_mha_binary_unbalanced/\",\n",
    "                best_predefined_f1=0.72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_env",
   "language": "python",
   "name": "keras_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
